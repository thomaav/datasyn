#+TITLE: TDT4265 - Computer Vision and Deep Learning Assignment 2
#+AUTHOR: Thomas Aven, Lasse Eggen
#+EXPORT_FILE_NAME: tdt4265_thomaav_lasseaeg
#+LATEX_CLASS: lasse2
#+LATEX_CLASS_OPTIONS: [abstract=off,oneside]
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil
#+OPTIONS: num:nil

* 1 - Softmax regression with backpropagation
** 1.1 - Mathematics
Notice that the quadratic cost function is written without
exponent. It should be:\\
$C = \sum\limits_{k} \frac{1}{2}(a_k - t_k)^2$

#+BEGIN_center
# #+ATTR_LATEX: :center :width 1.0\textwidth
[[file:mathhhhs-1.png]]
[[file:mathhhhs-2.png]]
#+END_center

** 1.2 - Vectorize computation
TODO

* 2 - MNIST Classification
** a) Description of training procedure
TODO: learning rate to reflect live runs.

#+BEGIN_SRC python
model = Model()
model.add_layer(64, Activations.sigmoid, size_of_input_layer)
model.add_layer(10, Activations.softmax)
model.train(mnist, epochs=15, batch_size=128, lr=0.5, evaluate=True)
#+END_SRC

We split MNIST into a training and validation set. 10% of the dataset
is selected as validation set. Log-cross entropy is used. No
regularization is implemented (yet, as we'll dropout for task
5). Early stopping was not used, as we simply observed the training
behaviour over several training sessions and decided that 15 epochs
was sufficient. Evaluation is done 20 times per epoch.

Description of the model is shown in the python code above. The model
contains an input layer, a hidden layer with sigmoid and an output
layer with softmax. (784, 64, 10) neurons in the layers.

*** Hyperparameters
The hyperparameters are given to the model when kicking off training,
they are:

#+BEGIN_SRC python
model.train(mnist, epochs=15, batch_size=128, lr=0.5, evaluate=True)
#+END_SRC


** b) Numerical approximation check of gradients
TODO

** c) Plots of loss and accuracy
[[file:loss.png]]
[[file:accuracy.png]]



* 3 - Adding the "Tricks of the Trade"

#+CAPTION: (training accuracy, validation accuracy, test accuracy) in epoch [1, 5, 10, 15]
|   | 1                  | 5                  | 10                 | 15                 |
|---+--------------------+--------------------+--------------------+--------------------|
| B | (85.9, 85.3, 86.0) | (92.0, 91.6, 91.6) | (93.8, 93.0, 92.8) | (94.8, 93.4, 93.5) |
| S | (85.6, 85.0, 86.1) | (92.4, 91.0, 91.7) | (94.2, 92.4, 93.0) | (95.3, 93.6, 93.7) |
| I | (81.9, 81.3, 82.0) | (91.8, 91.0, 91.3) | (93.5, 92.1, 92.9) | (94.6, 93.5, 93.7) |
| N | (94.5, 94.3, 94.1) | (97.3, 96.1, 96.5) | (98.1, 96.4, 96.9) | (98.8, 96.7, 97.1) |
| M | (93.8, 93.2, 93.6) | (96.9, 95.4, 96.0) | (98.4, 96.4, 97.0) | (98.8, 96.4, 97.0) |
The labels in the table represent [B]ase, [S]huffle, [I]mproved
Sigmoid, [N]ormal distribution, [M]omentum ($\mu = 0.9$).




* 4 - Experiment with network topology
We use the same hyperparameters as before and the tricks of the trade,
without momentum.

** a, b, c) Halving and doubling the number of hidden units

| Neurons | 1                  | 5                  | 10                 | 15                 |
|---------+--------------------+--------------------+--------------------+--------------------|
|       5 | (83.2, 83.0, 83.0) | (85.0, 84.0, 84.8) | (85.9, 85.1, 85.1) | (85.6, 84.1, 84.7) |
|      10 | (89.5, 89.3, 89.4) | (91.1, 90.3, 90.8) | (91.5, 90.7, 90.9) | (92.4, 91.6, 91.5) |
|      20 | (91.1, 91.3, 91.3) | (93.3, 93.1, 93.3) | (95.1, 94.6, 93.9) | (95.7, 95.0, 94.7) |
|      32 | (92.7, 92.3, 92.6) | (94.9, 93.9, 94.4) | (97.1, 95.3, 95.8) | (97.4, 95.2, 95.9) |
|---------+--------------------+--------------------+--------------------+--------------------|
|      64 | (94.5, 94.3, 94.1) | (97.3, 96.1, 96.5) | (98.1, 96.4, 96.9) | (98.8, 96.7, 97.1) |
|---------+--------------------+--------------------+--------------------+--------------------|
|     128 | (93.7, 93.3, 93.6) | (97.6, 96.4, 96.6) | (98.2, 96.8, 96.9) | (99.5, 97.3, 97.5) |
|  60, 60 | (94.4, 93.8, 94.1) | (97.7, 96.9, 96.5) | (98.5, 96.9, 96.8) | (98.8, 97.0, 96.8) |
|    128* |                    |                    |                    |                    |
*: ReLU, Dropout.

** c) Increase number of hidden layers
With one hidden layer the architecture has $764*64 + 10*64 = 49536$
weights. To approximate this number of weights, we find that 60
neurons in each of the hidden layers gives about the same total number
of weights in the network, $764*60 + 60*60 + 60*10 = 50040$.

# #+BEGIN_center
# #+ATTR_LATEX: :center :width 1.0\textwidth
# [[./linreglosses.png]]
# #+END_center
